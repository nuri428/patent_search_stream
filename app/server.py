import sys
from pathlib import Path
import os

# ì ˆëŒ€ ê²½ë¡œë¡œ ì§€ì •
PACKAGE_DIR = Path(__file__).resolve().parent.parent / "pkgs" / "kipris_tools"
sys.path.insert(0, str(PACKAGE_DIR))

# ë””ë²„ê¹…ì„ ìœ„í•œ ì¶œë ¥
print("PYTHONPATH:", os.environ['PYTHONPATH'])
print("sys.path:", sys.path)

import langchain_kipris_tools

import streamlit as st

from streamlit_chat import message
# from services.patent.api import react_agent_executor
# from services.simple_langgraph.graph import app
# from services.simple_langgraph2.graph import research_chain
from services.patent.api.simplachain import call_with_tool
import os
import uuid
from langchain_teddynote import logging
logging.langsmith('patent_search')
# from icecream import ic
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = st.secrets["LANGCHAIN"]["endpoint"]
os.environ["LANGCHAIN_API_KEY"] = st.secrets["LANGCHAIN"]["api_key"]
os.environ["LANGCHAIN_PROJECT"] = st.secrets["LANGCHAIN"]["project"]
os.environ["LANGSMITH_API_KEY"] = st.secrets["LANGCHAIN"]["api_key"]
os.environ["TAVILY_API_KEY"] = st.secrets["TAVILY"]["api_key"]
st.set_page_config(page_title="ğŸ¤—ğŸ’¬ patent_chatbot", layout="wide")
if "session_id" not in st.session_state.keys():
    st.session_state.session_id = str(uuid.uuid4())
# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": " ì•ˆë…•í•˜ì„¸ìš” ë­˜ ë„ì™€ ë“œë¦´ê¹Œìš”?"}]

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# Function for generating LLM response
def generate_response(prompt_input: str) -> str:
    if not prompt_input:
        return "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    try:
        res = call_with_tool(prompt_input)
        return res
    except Exception as e:
        print(f"Error in generate_response: {str(e)}")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response(prompt)          
            st.markdown(response)

    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)