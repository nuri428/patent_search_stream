import asyncio
import os
import sys
import uuid
from pathlib import Path

import streamlit as st
from langchain_teddynote import logging
# from services.patent.api.simplachain import call_with_tool_stream
from services.patent.simple_graph.graph import PatentSearcherGraph
from langchain.schema import HumanMessage
# íŒ¨í‚¤ì§€ ê²½ë¡œ ì„¤ì •
PACKAGE_DIR = Path(__file__).resolve().parent.parent / "pkgs" / "kipris_tools"
sys.path.insert(0, str(PACKAGE_DIR))

# LangSmith ì„¤ì •
logging.langsmith('patent_search')
for key, value in {
    "LANGCHAIN_TRACING_V2": "true",
    "LANGCHAIN_ENDPOINT": st.secrets["LANGCHAIN"]["endpoint"],
    "LANGCHAIN_API_KEY": st.secrets["LANGCHAIN"]["api_key"],
    "LANGCHAIN_PROJECT": st.secrets["LANGCHAIN"]["project"],
    "LANGSMITH_API_KEY": st.secrets["LANGCHAIN"]["api_key"],
    "TAVILY_API_KEY": st.secrets["TAVILY"]["api_key"]
}.items():
    os.environ[key] = value

# Streamlit ì„¤ì •
st.set_page_config(page_title="ğŸ¤—ğŸ’¬ patent_chatbot", layout="wide")

class ChatProcessor:
    def __init__(self):
        self.graph = PatentSearcherGraph()
        self.messages = st.session_state.messages
        
    def add_message(self, role: str, content: str):
        """ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€"""
        self.messages.append({"role": role, "content": content})
        
    def display_message(self, role: str, content: str):
        """ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œ"""
        with st.chat_message(role):
            st.write(content)

    async def process_response(self, prompt: str, streaming: bool = True):
        """ì‘ë‹µ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³µí†µ í•¨ìˆ˜"""
        if not prompt:
            return "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
        try:
            if streaming:
                return await self._process_streaming(prompt)
            else:
                return await self._process_sync(prompt)
        except Exception as e:
            print(f"Error in process_response: {str(e)}")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
    async def _process_streaming(self, prompt: str) -> str:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
        response_container = st.empty()
        full_response = []
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ í¬í•¨
        async for chunk in self.graph.run_stream(prompt):
            full_response.append(chunk)
            response_container.write("".join(full_response))
            
        return "".join(full_response)
        
    async def _process_sync(self, prompt: str) -> str:
        """ë™ê¸°ì‹ ì‘ë‹µ ì²˜ë¦¬"""
        return await self.graph.invoke({"messages": [HumanMessage(content=prompt)]})
        # return await call_with_tool_stream(prompt).__anext__()

# Streamlit ì•± ì´ˆê¸°í™”
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš” ë­˜ ë„ì™€ ë“œë¦´ê¹Œìš”?"}]
if "chat_processor" not in st.session_state:
    st.session_state.chat_processor = ChatProcessor()

# ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    processor = st.session_state.chat_processor
    processor.add_message("user", prompt)
    processor.display_message("user", prompt)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = asyncio.run(processor.process_response(prompt))
            processor.add_message("assistant", response)